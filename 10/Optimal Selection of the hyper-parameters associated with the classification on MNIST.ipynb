{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optimal Selection of the hyper-parameters associated with the classification on MNIST.ipynb","provenance":[],"authorship_tag":"ABX9TyMWCEGOEDkmMNQmQCrI4wqt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"O1305PAEniJS"},"source":["# 0. Setting\n","<hr>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ZjRuljDPm4ML","executionInfo":{"status":"ok","timestamp":1606110284669,"user_tz":-540,"elapsed":784,"user":{"displayName":"장예솔","photoUrl":"","userId":"10358588701743689220"}},"outputId":"27a21e7c-f7ca-43b6-fa28-e00d5639025b"},"source":["# import library\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import math\n","from pandas import Series, DataFrame\n","import pandas as pd\n","import numpy as np\n","\n","torch.__version__\n"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.7.0+cu101'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"QPJ7yRhanyqJ"},"source":["# 1. Data\n","<hr>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0I9AxJjnzpe","executionInfo":{"status":"ok","timestamp":1606111708791,"user_tz":-540,"elapsed":833,"user":{"displayName":"장예솔","photoUrl":"","userId":"10358588701743689220"}},"outputId":"6996a6e4-900f-461d-9331-1e8163826374"},"source":["from torchvision import transforms, datasets\n","\n","data_path = './MNIST'\n","\n","data_train  = datasets.MNIST(root = data_path, train= False, download=True)\n","data_test = datasets.MNIST(root = data_path, train= True, download=True)\n","\n","data_train_mean = data_train.data.float().mean()/255\n","data_train_std = data_train.data.float().std()/255\n","\n","data_test_mean = data_test.data.float().mean()/255\n","data_test_std = data_test.data.float().std()/255\n","\n","\n","print(\"train data mean = {}, std = {}\".format(data_train_mean, data_train_std))\n","print(\"test data mean = {}, std = {}\".format(data_test_mean, data_test_std))\n","\n","\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((data_train_mean,),(data_train_std,)),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((data_test_mean,),(data_test_std,)),\n","])\n","\n","data_train  = datasets.MNIST(root = data_path, train= False, download=True, transform= test_transform)\n","data_test   = datasets.MNIST(root = data_path, train= True, download=True, transform= train_transform)\n","\n","print(\"the number of your training data (must be 10,000) = \", data_train.__len__())\n","print(\"hte number of your testing data (must be 60,000) = \", data_test.__len__())\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["train data mean = 0.1325146108865738, std = 0.3104802668094635\n","test data mean = 0.13066047430038452, std = 0.30810779333114624\n","the number of your training data (must be 10,000) =  10000\n","hte number of your testing data (must be 60,000) =  60000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MeGoRsbdyU35"},"source":["# 2.Model\n","<hr>"]},{"cell_type":"code","metadata":{"id":"veKdDvFAoUpW","executionInfo":{"status":"ok","timestamp":1606113278877,"user_tz":-540,"elapsed":871,"user":{"displayName":"장예솔","photoUrl":"","userId":"10358588701743689220"}}},"source":["class classification(nn.Module):\n","    def __init__(self):\n","        super(classification, self).__init__()\n","        \n","        # construct layers for a neural network\n","        self.classifier1 = nn.Sequential(\n","            nn.Linear(in_features=28*28, out_features=20*20),\n","            nn.ReLU(),\n","        ) \n","        self.classifier2 = nn.Sequential(\n","            nn.Linear(in_features=20*20, out_features=10*10),\n","            nn.ReLU(),\n","        ) \n","        self.classifier3 = nn.Sequential(\n","            nn.Linear(in_features=10*10, out_features=10),\n","            nn.ReLU(),\n","        ) \n","\n","        \n","        \n","    def forward(self, inputs):                 # [batchSize, 1, 28, 28]\n","        x = inputs.view(inputs.size(0), -1)    # [batchSize, 28*28]\n","        x = self.classifier1(x)                # [batchSize, 20*20]\n","        x = self.classifier2(x)                # [batchSize, 10*10]\n","        out = self.classifier3(x)              # [batchSize, 10]\n","        \n","        return out\n"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23655Ihs0RKc"},"source":["# 3. Loss Function\n","<hr>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03QBZh2w0Yas","executionInfo":{"status":"ok","timestamp":1606114710406,"user_tz":-540,"elapsed":860,"user":{"displayName":"장예솔","photoUrl":"","userId":"10358588701743689220"}},"outputId":"1f1c0357-a24b-4508-9d52-4d332826ec98"},"source":["model = classification()\n","criterion = nn.CrossEntropyLoss()\n","train_y_pred = model(data_train.data.float())\n","train_y = data_train.targets\n","temp_loss = criterion(train_y_pred, train_y)\n","print(temp_loss)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["tensor(8.6880, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7h4nHZPm4fkd"},"source":[""],"execution_count":null,"outputs":[]}]}